{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_and_ResNetipynb",
      "provenance": [],
      "collapsed_sections": [
        "R797_JBH8jxP"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjGRYaFnyCow"
      },
      "source": [
        "# **Diagnosing Diabetic Retinopathy**\n",
        "### **Ben Chrepta and Fariha Tamboli**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWw7Shf-Kz5W"
      },
      "source": [
        "#Part 0: Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RHX0vmeKzRo"
      },
      "source": [
        "#full imports on Fariha-FinalProject"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3P7kcGL2yJRl"
      },
      "source": [
        "# Part 1: Load the Dataset from Kaggle\n",
        "For some reason, you have to run this twice to get it to work\n",
        "\n",
        "Reference: https://towardsdatascience.com/setting-up-kaggle-in-google-colab-ebb281b61463 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9R43tXoly8QD"
      },
      "source": [
        "#source: https://towardsdatascience.com/setting-up-kaggle-in-google-colab-ebb281b61463\n",
        "!pip install --upgrade kaggle\n",
        "!mkdir .kaggle\n",
        "\n",
        "import json\n",
        "token = {'username':'ftamboli','key':'f332d8730eb66bc382fdd2c2167c658b'}\n",
        "with open('/content/.kaggle/kaggle.json', 'w') as file:\n",
        "    json.dump(token, file)\n",
        "\n",
        "!cp /content/.kaggle/kaggle.json ~/.kaggle/kaggle.json\n",
        "!kaggle config set -n path -v{/content}\n",
        "\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "!kaggle datasets list\n",
        "\n",
        "!kaggle datasets list -s dr\n",
        "\n",
        "!kaggle datasets download -d tanlikesmath/diabetic-retinopathy-resized -p /content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCWKk8P8z5my"
      },
      "source": [
        "!unzip \\*.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-s5VgG_SLRQK"
      },
      "source": [
        "#Part 2: Formatting/Visualizing the Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlXSf_EyLXDM"
      },
      "source": [
        "#0 is no DR, 1 is DR (of any level)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "df_train = pd.read_csv('/content/trainLabels_nice.csv')\n",
        "\n",
        "df_train['level'] = df_train['level'].apply(lambda x: 1 if x!= 0 else 0)\n",
        "\n",
        "print(df_train.columns)\n",
        "\n",
        "x = df_train['image']\n",
        "y = df_train['level']\n",
        "\n",
        "x, y = shuffle(x, y, random_state=75)\n",
        "\n",
        "train_x, valid_x, train_y, valid_y = train_test_split(x, y, test_size=0.2,\n",
        "                                                      stratify=y, random_state=75)\n",
        "print(train_x.shape, train_y.shape, valid_x.shape, valid_y.shape)\n",
        "train_y.hist()\n",
        "valid_y.hist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfBc4s270W88"
      },
      "source": [
        "##Crop images\n",
        "\n",
        "Here, we introduce three different ideas for cropping our images. We will choose the bright crop, since there is more contrast in the images.\n",
        "\n",
        "\n",
        "Reference: These ideas are originally from @xhlulu on Kaggle. \n",
        "\n",
        "https://www.kaggle.com/xhlulu/aptos-2019-densenet-keras-starter\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mua1KBlGwRIi",
        "cellView": "form"
      },
      "source": [
        "#@title Simple Crop\n",
        "#@markdown Crop out all the black border\n",
        "fig = plt.figure(figsize=(25, 16))\n",
        "# display 10 images from each class\n",
        "for class_id in sorted(train_y.unique()):\n",
        "    for i, (idx, row) in enumerate(df_train.loc[df_train['level'] == class_id].sample(5, random_state=75).iterrows()):\n",
        "        ax = fig.add_subplot(5, 5, class_id * 5 + i + 1, xticks=[], yticks=[])\n",
        "        path=f\"/content/resized_train_cropped/resized_train_cropped/{row['image']}.jpeg\"\n",
        "        image = cv2.imread(path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        image = cv2.resize(image, (512,512))\n",
        "\n",
        "        plt.imshow(image)\n",
        "        ax.set_title('Label: %d-%d-%s' % (class_id, idx, row['level']) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "wXJusKpuykim"
      },
      "source": [
        "#@title Black and White crop\n",
        "#@markdown Build on the Simple Crop, then greyscale\n",
        "fig = plt.figure(figsize=(25, 16))\n",
        "for class_id in sorted(train_y.unique()):\n",
        "    for i, (idx, row) in enumerate(df_train.loc[df_train['level'] == class_id].sample(5, random_state=75).iterrows()):\n",
        "        ax = fig.add_subplot(5, 5, class_id * 5 + i + 1, xticks=[], yticks=[])\n",
        "        path=f\"/content/resized_train_cropped/resized_train_cropped/{row['image']}.jpeg\"\n",
        "        image = cv2.imread(path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        image = cv2.resize(image, (512,512))\n",
        "        image=cv2.addWeighted ( image,4, cv2.GaussianBlur( image , (0,0) , 512/10) ,-4 ,128) # the trick is to add this line\n",
        "\n",
        "        plt.imshow(image, cmap='gray')\n",
        "        ax.set_title('Label: %d-%d-%s' % (class_id, idx, row['image']) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "6jPEEFgyzIbl"
      },
      "source": [
        "#@title Bright Crop\n",
        "#@markdown Change the illumination. Build on the Simple Crop\n",
        "def crop_image1(img,tol=7):\n",
        "    # img is image data\n",
        "    # tol  is tolerance\n",
        "        \n",
        "    mask = img>tol\n",
        "    return img[np.ix_(mask.any(1),mask.any(0))]\n",
        "\n",
        "def crop_image(img,tol=7):\n",
        "    if img.ndim ==2:\n",
        "        mask = img>tol\n",
        "        return img[np.ix_(mask.any(1),mask.any(0))]\n",
        "    elif img.ndim==3:\n",
        "        h,w,_=img.shape\n",
        "#         print(h,w)\n",
        "        img1=cv2.resize(crop_image1(img[:,:,0]),(w,h))\n",
        "        img2=cv2.resize(crop_image1(img[:,:,1]),(w,h))\n",
        "        img3=cv2.resize(crop_image1(img[:,:,2]),(w,h))\n",
        "        \n",
        "#         print(img1.shape,img2.shape,img3.shape)\n",
        "        img[:,:,0]=img1\n",
        "        img[:,:,1]=img2\n",
        "        img[:,:,2]=img3\n",
        "        return img\n",
        "\n",
        "def load_ben_color(path, sigmaX=10):\n",
        "    image = cv2.imread(path)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    image = crop_image(image)\n",
        "    image = cv2.resize(image, (512,512))\n",
        "    image=cv2.addWeighted ( image,4, cv2.GaussianBlur( image , (0,0) , sigmaX) ,-4 ,128)\n",
        "    # print(type(image))\n",
        "    return image\n",
        "\n",
        "NUM_SAMP=7\n",
        "fig = plt.figure(figsize=(25, 16))\n",
        "for class_id in sorted(train_y.unique()):\n",
        "    for i, (idx, row) in enumerate(df_train.loc[df_train['level'] == class_id].sample(NUM_SAMP, random_state=75).iterrows()):\n",
        "        ax = fig.add_subplot(5, NUM_SAMP, class_id * NUM_SAMP + i + 1, xticks=[], yticks=[])\n",
        "        path=f\"/content/resized_train_cropped/resized_train_cropped/{row['image']}.jpeg\"\n",
        "        image = load_ben_color(path,sigmaX=30)\n",
        "\n",
        "        plt.imshow(image)\n",
        "        ax.set_title('%d-%d-%s' % (class_id, idx, row['image']) )\n",
        "\n",
        "\n",
        "#create new file with all the images\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXDxKR3FKoec"
      },
      "source": [
        "##Create the Bright Crop images, make a GDrive folder, do not need to run this (its already done)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVHcIzQsDG5e",
        "cellView": "form"
      },
      "source": [
        "#@title Choose Bright Crop, Create new file of bright crop images\n",
        "#@markdown This does not need to be run. This takes more than 2 hours. This is the link of the file of converted images: https://drive.google.com/drive/folders/1AutCwMZ7T0VXl7CrOAmgJ7ATdcoLSw2s?usp=sharing \n",
        "#Run time: 2.25 hours\n",
        "\n",
        "# import shutil\n",
        "# shutil.rmtree('/content/bright_images')\n",
        "\n",
        "!mkdir bright_images\n",
        "\n",
        "root_dir = '/content/'\n",
        "content_dir = 'resized_train_cropped/resized_train_cropped/'\n",
        "for f in df_train['image']:\n",
        "   file_name = content_dir + f + '.jpeg'\n",
        "   img_name = os.path.join(root_dir, file_name)\n",
        "   image = load_ben_color(img_name,sigmaX=30)\n",
        "   \n",
        "   cv2.imwrite(\"/content/bright_images/\"+ f + \".jpeg\", image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bF15R4IeNupy"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "thcBdsRcuG_B"
      },
      "source": [
        "#@title Take the generated images from above and put them into the google drive folder.\n",
        "#@markdown This does not need to be run, since this is already done. \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "root_dir = '/content/'\n",
        "content_dir = 'bright_images/'\n",
        "for f in df_train['image']:\n",
        "  file_name = content_dir+f+'.jpeg'\n",
        "  img_name = os.path.join(root_dir, file_name)\n",
        "  print(img_name)\n",
        "  image = io.imread(img_name)\n",
        "  cv2.imwrite('/content/drive/My Drive/drive_bright_images/' + f + '.jpeg' , image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQNATo4K512g"
      },
      "source": [
        "#Part 3: Loading Images\n",
        "Create a dataset class, and pass it into a dataloader. \n",
        "\n",
        "Reference: https://pytorch.org/tutorials/beginner/data_loading_tutorial.html "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2onLU6bh59Cu"
      },
      "source": [
        "files = os.listdir('/content/resized_train_cropped/resized_train_cropped/')\n",
        "print(len(files))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0hBZqun6PXc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "cellView": "form",
        "outputId": "8fac1c03-5068-48fc-cb79-39b4c7b444ef"
      },
      "source": [
        "#@title Visualizing the Distribution of Data\n",
        "# base_image_dir = os.path.join('/content', '/')\n",
        "df = pd.read_csv(os.path.join('/content/', 'trainLabels_nice.csv'))\n",
        "df['path'] = df['image'].map(lambda x: os.path.join('/content/drive/MyDrive/drive_bright_images','{}.jpeg'.format(x)))\n",
        "df = df.drop(columns=['image'])\n",
        "df = df.sample(frac=1).reset_index(drop=True) #shuffle dataframe\n",
        "df['level'] = (df['level'] > 0).astype(int) # Disease or no disease\n",
        "df.head(10)\n",
        "train_df, val_df = train_test_split(df,test_size=0.2)\n",
        "df['level'].hist(figsize = (10, 5))\n",
        "train_df['level'].hist(figsize = (10, 5))\n",
        "len(val_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "164"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F49PH8Ua1K1D"
      },
      "source": [
        "print(df_train)\n",
        "print(df_train['level'][5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oi13p4KoWeqe"
      },
      "source": [
        "#title Dataloaders\n",
        "dr_data = DRDataset(csv_file='trainLabels_cropped.csv',\n",
        "                            root_dir='/content/drive/My Drive/drive_bright_images')\n",
        "print(len(dr_data))\n",
        "len_dr = len(dr_data)\n",
        "train_size = (int)(0.8*len_dr)\n",
        "test_size = len_dr - train_size\n",
        "train_dr, test_dr = torch.utils.data.random_split(dr_data, [train_size, test_size])\n",
        "\n",
        "\n",
        "\n",
        "batch_size = 10\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dr, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dr, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "\n",
        "fig = plt.figure()\n",
        "\n",
        "print(dr_data)\n",
        "for i in range(len(dr_data)):\n",
        "    image, label = dr_data[i]\n",
        "\n",
        "    print(image.shape, label.shape, \"label: \", label)\n",
        "\n",
        "    ax = plt.subplot(1, 6, i + 1)\n",
        "    plt.tight_layout()\n",
        "    ax.set_title('Sample #{}'.format(i))\n",
        "    ax.axis('off')\n",
        "    sample = {'image':image, 'label': label}\n",
        "    show_landmarks(**sample)\n",
        "\n",
        "    if i == 5:\n",
        "        plt.show()\n",
        "        break\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLDSHKXogkoB"
      },
      "source": [
        "#Part 4: Non-Deep Learning Benchmark: Logistic Regression Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8klzOAOEY1C"
      },
      "source": [
        "#on DataProccessing_and_LogisticRegression.ipynb linked below\n",
        "#https://colab.research.google.com/drive/1EeOpq2oOPrqxYYFDQ81MtyQ-FBdNgOmE?usp=sharing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owMLzh4lJ1mV"
      },
      "source": [
        "#Part 5: CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBPAwVKRJ_4H"
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "import os\n",
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "class DRDataset(Dataset):\n",
        "    def __init__ (self, csv_file, root_dir, transform=None):\n",
        "        \"\"\"\n",
        "        csv_file = labels\n",
        "        root_dir = images\n",
        "        \n",
        "        \"\"\"\n",
        "        \n",
        "        self.train_data_csv = pd.read_csv(csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "    def __len__(self):\n",
        "        return len(self.train_data_csv)\n",
        "\n",
        "    def __getitem__ (self, idx): \n",
        "        name = self.train_data_csv.iloc[idx, 2]\n",
        "        img_name = os.path.join(self.root_dir, name +'.jpeg')\n",
        "        image = Image.open(img_name)\n",
        "        y_labels = float(self.train_data_csv.iloc[idx, 3:])\n",
        "        if y_labels == 0:\n",
        "          y_labels = np.array([1,0])\n",
        "        elif y_labels ==1:\n",
        "          y_labels = np.array([0,1])\n",
        "        else: pass\n",
        "        \n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "        return (image, y_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgE3mYmutdJa"
      },
      "source": [
        "\n",
        "trainer_names_csv = pd.read_csv('/content/trainLabels_nice.csv')\n",
        "\n",
        "n = len(trainer_names_csv)\n",
        "img_names = trainer_names_csv.iloc[:n, 2]\n",
        "label = trainer_names_csv.iloc[:n, 3]\n",
        "\n",
        "print(img_names[0])\n",
        "print(label[1])\n",
        "def showimages(img):\n",
        "    plt.imshow(img)\n",
        "    \n",
        "showimages(io.imread('/content/drive/MyDrive/drive_bright_images/' + img_names[0] + '.jpeg'))\n",
        "print(f'Label: {label[0]}, Image_ID: {img_names[0]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vL8hicK-r2ON"
      },
      "source": [
        "\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "num_epochs = 10\n",
        "learning_rate = 0.001\n",
        "train_CNN = True\n",
        "batch_size = 10\n",
        "shuffle = True\n",
        "pin_memory = True\n",
        "num_workers = 0\n",
        "\n",
        "\n",
        "\n",
        "train_size = int(0.8 * n)\n",
        "validation_size = n - train_size\n",
        "\n",
        "\n",
        "image_dataset = DRDataset(csv_file='/content/trainLabels_nice.csv', root_dir='/content/drive/MyDrive/drive_bright_images/')\n",
        "\n",
        "dataset = DRDataset(csv_file='/content/trainLabels_nice.csv', root_dir='/content/drive/MyDrive/drive_bright_images/', \n",
        "                               transform=transform)\n",
        "\n",
        "\n",
        "train_set, validation_set = torch.utils.data.random_split(dataset, [train_size, validation_size])\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_set, shuffle=shuffle, batch_size=batch_size,num_workers=num_workers, pin_memory=pin_memory)\n",
        "validation_loader = torch.utils.data.DataLoader(dataset=validation_set, shuffle=shuffle, batch_size=batch_size,num_workers=num_workers, pin_memory=pin_memory)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "dataiter = iter(train_loader)\n",
        "image, label = dataiter.next()\n",
        "\n",
        "print(image.shape)\n",
        "print(label.shape)\n",
        "print(label)\n",
        "print(image)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVOpNXxp6Isa"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__ (self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, 5)\n",
        "        self.pool = nn.MaxPool2d(4, 4)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 5)\n",
        "        self.pool = nn.MaxPool2d(4, 4)\n",
        "        self.conv3 = nn.Conv2d(64, 128, 5)\n",
        "        self.pool = nn.MaxPool2d(4, 3)\n",
        "\n",
        "        self.fc1 = nn.Linear(6272, 512)\n",
        "        self.fc2 = nn.Linear(512, 64)\n",
        "\n",
        "        self.fc3 = nn.Linear(64, 2)\n",
        "       \n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "        self.flat = nn.Flatten()\n",
        "\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.flat(x)\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "    \n",
        "net = Net()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ml3_Xf1bV1wk"
      },
      "source": [
        "iter = 0\n",
        "total_loss = []\n",
        "total_acc = []\n",
        "epochs = 10\n",
        "import torch.optim as optim  \n",
        "criterion = nn.BCEWithLogitsLoss() \n",
        "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "for epoch in range(int(epochs)):\n",
        "    print(\"epoch num: \", epoch)\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    correct = 0\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = Variable(images.view(images.shape[0], -1)) #, requires_grad = True)\n",
        "        labels = Variable(labels.squeeze().type(torch.DoubleTensor)) #, requires_grad = True)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(outputs.type(torch.DoubleTensor), labels)\n",
        "        loss = Variable(loss, requires_grad = True)\n",
        "       \n",
        "        epoch_loss+=loss.item()\n",
        "        outputs = net(inputs) \n",
        "        labels = labels.view(-1, 1)\n",
        "\n",
        "        loss = criterion(outputs, labels) \n",
        "        _,predicted = torch.max(outputs.data, 1)\n",
        "        new_labels = predicted.clone()\n",
        "        int_it = 0\n",
        "        for l in labels:\n",
        "          p = l.data.cpu().numpy()[0]\n",
        "          # print(\"p is: \", p)\n",
        "          if p == 1:\n",
        "            new_labels[int_it] = 0.\n",
        "          elif p == 0:\n",
        "            new_labels[int_it] = 1.\n",
        "          else: pass\n",
        "          int_it+=1\n",
        "\n",
        "        \n",
        "        correct = (predicted == new_labels).sum()\n",
        "        epoch_acc += correct.data.cpu().data.tolist()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        iter+=1\n",
        "    total_loss.append(epoch_loss/len(train_loader))\n",
        "    total_acc.append(epoch_acc/len(train_loader))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LECE4g8V1Mp"
      },
      "source": [
        "print(epoch_loss)\n",
        "print(total_loss)\n",
        "print(epoch_acc)\n",
        "print(total_acc)\n",
        "\n",
        "lr_model.eval()\n",
        "lr_model.to(device)\n",
        "epochs = 10\n",
        "plt.plot(np.linspace(1, epochs, epochs).astype(int), total_loss)\n",
        "\n",
        "\n",
        "lr_model.eval()\n",
        "lr_model.to(device)\n",
        "epochs = 10\n",
        "plt.plot(np.linspace(1, epochs, epochs).astype(int), total_acc)\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXhUjiHXy6SN"
      },
      "source": [
        "#Part 6: ResNet\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rPq72ExKCMy"
      },
      "source": [
        "\n",
        "trainer_names_csv = pd.read_csv('/content/trainLabels_nice.csv')\n",
        "\n",
        "n = len(trainer_names_csv)\n",
        "img_names = trainer_names_csv.iloc[:n, 2]\n",
        "label = trainer_names_csv.iloc[:n, 3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBFJOhQrzBvq"
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "import os\n",
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "class DRDataset(Dataset):\n",
        "    def __init__ (self, csv_file, root_dir, transform=None):\n",
        "        \"\"\"\n",
        "        csv_file = labels\n",
        "        root_dir = images\n",
        "        \n",
        "        \"\"\"\n",
        "        \n",
        "        self.train_data_csv = pd.read_csv(csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "    def __len__(self):\n",
        "        return len(self.train_data_csv)\n",
        "\n",
        "    def __getitem__ (self, idx): \n",
        "        name = self.train_data_csv.iloc[idx, 2]\n",
        "        img_name = os.path.join(self.root_dir, name +'.jpeg')\n",
        "        image = Image.open(img_name)\n",
        "        y_labels = float(self.train_data_csv.iloc[idx, 3:])\n",
        "        if y_labels == 0:\n",
        "          y_labels = np.array([1,0])\n",
        "        elif y_labels ==1:\n",
        "          y_labels = np.array([0,1])\n",
        "        else: pass\n",
        "        \n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "        return (image, y_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gM_KXK2-8RGO"
      },
      "source": [
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()   \n",
        "])\n",
        "\n",
        "num_epochs = 10\n",
        "learning_rate = 0.001\n",
        "train_CNN = True\n",
        "batch_size = 10\n",
        "shuffle = True\n",
        "pin_memory = True\n",
        "num_workers = 0\n",
        "\n",
        "\n",
        "\n",
        "train_size = int(0.8 * n)\n",
        "validation_size = n - train_size\n",
        "\n",
        "\n",
        "image_dataset = DRDataset(csv_file='/content/trainLabels_nice.csv', root_dir='/content/drive/MyDrive/drive_bright_images/')\n",
        "\n",
        "dataset = DRDataset(csv_file='/content/trainLabels_nice.csv', root_dir='/content/drive/MyDrive/drive_bright_images/', \n",
        "                               transform=transform)\n",
        "\n",
        "train_set, validation_set = torch.utils.data.random_split(dataset, [train_size, validation_size])\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_set, shuffle=shuffle, batch_size=batch_size,num_workers=num_workers, pin_memory=pin_memory)\n",
        "validation_loader = torch.utils.data.DataLoader(dataset=validation_set, shuffle=shuffle, batch_size=batch_size,num_workers=num_workers, pin_memory=pin_memory)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "dataiter = iter(train_loader)\n",
        "image, label = dataiter.next()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYChjYAA8Y_E"
      },
      "source": [
        "from __future__ import print_function, division\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import cv2\n",
        "\n",
        "plt.ion()\n",
        "\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.ToTensor()\n",
        "    ]),\n",
        "}\n",
        "\n",
        "\n",
        "train_set, validation_set = torch.utils.data.random_split(dataset, [train_size, validation_size])\n",
        "image_dataset = {'train': train_set, 'val': validation_set}\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_dataset[x], batch_size=batch_size, shuffle=True) for x in ['train', 'val']} \n",
        "dataset_sizes = {x: len(image_dataset[x]) for x in ['train', 'val']}\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "dataiters = iter(dataloaders['train'])\n",
        "images, labels = dataiters.next()\n",
        "\n",
        "\n",
        "total_loss = []\n",
        "total_acc = []\n",
        "\n",
        "def train_model(model, criterion, optimizer, scheduler, num_epochs):\n",
        "    best_models_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch}/{num_epochs-1}')\n",
        "        print('-' * 10)\n",
        "        \n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train() \n",
        "            else: \n",
        "                model.eval()\n",
        "        \n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs.cuda(), labels.type(torch.LongTensor).cuda())\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                running_loss =+ loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss/dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double()/dataset_sizes[phase]\n",
        "            total_loss.append(epoch_loss)\n",
        "            total_acc.append(epoch_acc)\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXEU-TLZRIqH"
      },
      "source": [
        "def visualize_model(model, num_images=6): \n",
        "    was_training = mode.training\n",
        "    model.eval()\n",
        "    images_so_far = 0\n",
        "    fig = plt.figure()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            \n",
        "            for j in range(inputs.size()[0]):\n",
        "                images_so_far += 1\n",
        "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
        "                ax.axis('off')\n",
        "                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
        "                imshow(inputs.cpu().data[j])\n",
        "\n",
        "                if images_so_far == num_images:\n",
        "                    model.train(mode=was_training)\n",
        "                    return\n",
        "        model.train(mode=was_training)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYbkhqvBV2bG"
      },
      "source": [
        "\n",
        "print(models)\n",
        "model_ft = models.resnet18(pretrained=True)\n",
        "num_ftrs = model_ft.fc.in_features\n",
        "model_ft.fc = nn.Linear(num_ftrs, 2)\n",
        "\n",
        "model_ft = model_ft.to(\"cuda\")\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.001)\n",
        "\n",
        "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
        "                       num_epochs=25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3g_sywSgaZqy"
      },
      "source": [
        "print(total_loss)\n",
        "print(total_acc)\n",
        "\n",
        "trainL = []\n",
        "valL = []\n",
        "trainAcc =[]\n",
        "valAcc = []\n",
        "epochs = 10\n",
        "i = 0\n",
        "j = 0\n",
        "\n",
        "for x in total_loss:\n",
        "  if i % 2 == 0:\n",
        "    trainL.append(x)\n",
        "  else:\n",
        "    valL.append(x)\n",
        "  i = i + 1\n",
        "\n",
        "\n",
        "for y in total_acc:\n",
        "  if j % 2 == 0:\n",
        "    trainAcc.append(y)\n",
        "  else:\n",
        "    valAcc.append(y)\n",
        "  j = j + 1\n",
        "plt.plot(np.linspace(1, epochs, epochs).astype(int), trainL, label='Training Loss')\n",
        "plt.plot(np.linspace(1, epochs, epochs).astype(int), valL, label='Val Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('CNN: Epoch vs Loss')\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "390tBaJGhLCR"
      },
      "source": [
        "print(trainAcc)\n",
        "print(valAcc)\n",
        "plt.plot(np.linspace(1, epochs, epochs).astype(int), trainAcc, label='Training Accuracy')\n",
        "plt.plot(np.linspace(1, epochs, epochs).astype(int), valAcc, label='Val Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('CNN: Epochs vs Accuracy')\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R797_JBH8jxP"
      },
      "source": [
        "#Trash"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sR-x9pk6ybD"
      },
      "source": [
        "df = pd.concat([train_df,val_df])\n",
        "sz = 512\n",
        "bs = 16\n",
        "tfms = get_transforms(do_flip=True,flip_vert=True,max_rotate=360,max_warp=0,max_zoom=1.0,max_lighting=0.1,p_lighting=0.5)\n",
        "src = (ImageList.from_df(df=df,path='',cols='path') #get dataset from dataset\n",
        "        .split_by_idx(range(len(train_df)-1,len(df))) #Splitting the dataset\n",
        "        .label_from_df(cols='level') #obtain labels from the level column\n",
        "      )\n",
        "data= (src.transform(tfms,size=sz,resize_method=ResizeMethod.SQUISH,padding_mode='zeros') #Data augmentation\n",
        "        .databunch(bs=bs,num_workers=4) #DataBunch\n",
        "        .normalize(imagenet_stats) #Normalize     \n",
        "       )\n",
        "data.show_batch(rows=3, figsize=(7,6))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "Oq0BnCQ41WXJ"
      },
      "source": [
        "#@title Circle-ish Crop on Bright Crop\n",
        "def circle_crop(img, sigmaX=10):   \n",
        "    \"\"\"\n",
        "    Create circular crop around image centre    \n",
        "    \"\"\"    \n",
        "    \n",
        "    img = cv2.imread(img)\n",
        "    img = crop_image(img)    \n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    \n",
        "    height, width, depth = img.shape    \n",
        "    \n",
        "    x = int(width/2)\n",
        "    y = int(height/2)\n",
        "    r = np.amin((x,y))\n",
        "    \n",
        "    circle_img = np.zeros((height, width), np.uint8)\n",
        "    cv2.circle(circle_img, (x,y), int(r), 1, thickness=-1)\n",
        "    img = cv2.bitwise_and(img, img, mask=circle_img)\n",
        "    img = crop_image(img)\n",
        "    img=cv2.addWeighted ( img,4, cv2.GaussianBlur( img , (0,0) , sigmaX) ,-4 ,128)\n",
        "    return img \n",
        "\n",
        "NUM_SAMP=7\n",
        "fig = plt.figure(figsize=(25, 16))\n",
        "\n",
        "for class_id in sorted(train_y.unique()):\n",
        "    for i, (idx, row) in enumerate(df_train.loc[df_train['level'] == class_id].sample(NUM_SAMP, random_state=75).iterrows()):\n",
        "        ax = fig.add_subplot(5, NUM_SAMP, class_id * NUM_SAMP + i + 1, xticks=[], yticks=[])\n",
        "        path=f\"/content/resized_train_cropped/resized_train_cropped/{row['image']}.jpeg\"\n",
        "        image = circle_crop(path,sigmaX=30)\n",
        "          \n",
        "        plt.imshow(image)\n",
        "        ax.set_title('%d-%d-%s' % (class_id, idx, row['image']) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "kaT4lAwbwbWG",
        "outputId": "9e58e117-f86d-479c-ff30-ab74c84d09e5"
      },
      "source": [
        "print(type(train_loader))\n",
        "# helper.imshow(train_loader[0].to_numpy(), normalize = False)\n",
        "print(train_loader)\n",
        "images = iter(train_loader)\n",
        "helper.imshow(images[0], normalize=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'torch.utils.data.dataloader.DataLoader'>\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7f94657e6e10>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-0c9302258d70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mhelper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'helper' is not defined"
          ]
        }
      ]
    }
  ]
}